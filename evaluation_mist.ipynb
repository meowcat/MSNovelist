{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 25 15:36:30 2020\n",
    "\n",
    "@author: stravsm\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/msnovelist/fp_management/fp_database.py:62: UserWarning: PubChem database not found or not connected (read-only path?)\n",
      "  warn(\"PubChem database not found or not connected (read-only path?)\")\n",
      "03-Jun-24 13:54:42 - evaluation startup\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fp_management import database as db\n",
    "from fp_management import mist_fingerprinting as fpr\n",
    "from fp_management import base_fingerprinting as bfpr\n",
    "import smiles_config as sc\n",
    "\n",
    "sc.config_file.append(\"config.EULER-eval.yaml\")\n",
    "sc.config_reload()\n",
    "\n",
    "import infrastructure.generator as gen\n",
    "import infrastructure.decoder as dec\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "import infrastructure.score as msc\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Disable dropout. Is there a more elegant way to adapt config at runtime?\n",
    "sc.config[\"model_config\"][\"training\"] = False\n",
    "\n",
    "# Randomness is relevant for stochastic sampling\n",
    "random_seed = sc.config['random_seed_global']\n",
    "if random_seed != '':\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.experimental.set_seed(random_seed)\n",
    "\n",
    "# Setup logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MSNovelist\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"evaluation startup\")\n",
    "\n",
    "eval_folder = pathlib.Path(sc.config[\"eval_folder\"])\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_id = str(int(time.time()))\n",
    "pickle_id = eval_id\n",
    "if sc.config['eval_id'] != '':\n",
    "    eval_id = sc.config['eval_id']\n",
    "if sc.config['eval_counter'] != '':\n",
    "    pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "    \n",
    "if isinstance(sc.config['weights'], list):\n",
    "    weights_list = sc.config['weights']\n",
    "else:\n",
    "    weights_list = [sc.config['weights']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, do everything independent of weights\n",
    "\n",
    "fpr.MistFingerprinter.init_instance()\n",
    "fingerprinter = fpr.MistFingerprinter.get_instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "n = sc.config[\"eval_n\"]\n",
    "n_total = sc.config[\"eval_n_total\"]\n",
    "#n_total_ = n_total // n * n\n",
    "k = sc.config[\"eval_k\"]\n",
    "kk = sc.config[\"eval_kk\"]\n",
    "steps = sc.config[\"eval_steps\"]\n",
    "\n",
    "decoder_name = sc.config[\"decoder_name\"]\n",
    "\n",
    "evaluation_set = sc.config[\"evaluation_set\"]\n",
    "\n",
    "# File for CSI:FingerID validation data\n",
    "data_eval_ = sc.config[\"db_path_eval\"]\n",
    "# Load mapping table for the CSI:FingerID predictors\n",
    "# Load dataset and process appropriately\n",
    "db_eval = db.FpDatabase.load_from_config(data_eval_)\n",
    "pipeline_options =  db_eval.get_pipeline_options()\n",
    "    \n",
    "pipeline_encoder = sc.config['pipeline_encoder']\n",
    "pipeline_reference = sc.config['pipeline_reference']\n",
    "\n",
    "dataset_val = db_eval.get_grp(evaluation_set)\n",
    "if n_total != -1:\n",
    "    dataset_val = dataset_val[:n_total]\n",
    "else:\n",
    "    n_total = len(dataset_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_X': False,\n",
       " 'unpackbits': False,\n",
       " 'unpack': False,\n",
       " 'fingerprint_selected': 'fingerprint_degraded'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-the-fly translate the dataset :(\n",
    "# \n",
    "\n",
    "def entry_for_row(row):\n",
    "    res = {\n",
    "        key: row[key] for key in row.keys()\n",
    "    }\n",
    "    res[\"fingerprint\"] = fingerprinter.get_fp(row[\"fingerprint\"])[0,:]\n",
    "    res[\"fingerprint_degraded\"] = fingerprinter.get_fp(row[\"fingerprint_degraded\"])[0,:]\n",
    "    return res\n",
    "\n",
    "dataset_val_mapped = [entry_for_row(x) for x in dataset_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dataset_val_mapped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"fingerprint_degraded\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:54:43 - using unpickle_mf\n",
      "03-Jun-24 13:54:43 - not using unpack\n",
      "03-Jun-24 13:54:43 - not using fp_map\n",
      "03-Jun-24 13:54:44 - not using embed_X\n",
      "03-Jun-24 13:54:44 - Selecting fingerprint fingerprint_degraded\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Load dataset and sampler, apply sampler to dataset\n",
    "# (so we can also evaluate from fingerprint_sampled)\n",
    "fp_dataset_val_ = gen.smiles_pipeline(dataset_val_mapped,\n",
    "                                    batch_size = n,\n",
    "                                    **pipeline_options,\n",
    "                                    map_fingerprints=False,\n",
    "                                    degraded_fingerprint_type = \"uint8\")\n",
    "\n",
    "fp_dataset_val = gen.dataset_zip(fp_dataset_val_, \n",
    "                                 pipeline_encoder, pipeline_reference,\n",
    "                                 **pipeline_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fingerprint_selected': <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       "  'mol_form': <tf.Tensor: shape=(8, 10), dtype=float32, numpy=\n",
       "  array([[15.,  0.,  0.,  0.,  7.,  3.,  0.,  0.,  1., 19.],\n",
       "         [53.,  0.,  0.,  0.,  0., 22.,  0.,  0.,  0., 90.],\n",
       "         [47.,  0.,  0.,  0.,  2., 13.,  0.,  0.,  0., 56.],\n",
       "         [54.,  0.,  0.,  0.,  0., 23.,  0.,  0.,  0., 92.],\n",
       "         [16.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  1., 16.],\n",
       "         [21.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  0., 28.],\n",
       "         [23.,  0.,  0.,  2.,  7.,  1.,  0.,  0.,  0., 31.],\n",
       "         [21.,  1.,  0.,  0.,  2.,  2.,  0.,  0.,  1., 19.]], dtype=float32)>,\n",
       "  'n_hydrogen': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([19., 90., 56., 92., 16., 28., 31., 19.], dtype=float32)>},\n",
       " (<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
       "  array([b'CS(=O)(=O)N1CCc2c(-c3cnc(N)nc3)nc(N3CCOCC3)nc21',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OCC(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COC1C=COC2(C)Oc3c(C)c(OC(=O)c4ccc(C)cc4)c4c(c3C2=O)C(=O)C(N(C)C)=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(OC(C)=O)C1C)C4=O',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OC(CO)C(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COc1ccc(C2CC(c3ccsc3)=NN2C(C)=O)cc1O',\n",
       "         b'CCOc1ccc(N2C(=O)CC(N3CCCC4CCCCC43)C2=O)cc1',\n",
       "         b'CCC1CN(c2nc(N)c(C(N)=O)nc2Cl)CCN1C1CCN(Cc2ccc(Cl)cc2)CC1',\n",
       "         b'O=C(OCc1ccccc1)C1CCCN1c1nc(-c2ccc(F)cc2)cs1'], dtype=object)>,\n",
       "  <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(fp_dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:54:44 - Sampler basic_tp_fp loading\n",
      "03-Jun-24 13:54:45 - Sampler basic_tp_fp loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampler_name = sc.config['sampler_name']\n",
    "round_fingerprints = True\n",
    "if sampler_name != '':\n",
    "    logger.info(f\"Sampler {sampler_name} loading\")\n",
    "    sampler_module = importlib.import_module('fp_sampling.' + sampler_name, 'fp_sampling')\n",
    "    sampler_factory = sampler_module.SamplerFactory(sc.config)\n",
    "    round_fingerprints = sampler_factory.round_fingerprint_inference()\n",
    "    sampler = sampler_factory.get_sampler()\n",
    "    logger.info(f\"Sampler {sampler_name} loaded\")\n",
    "    fp_dataset_val_ = sampler.map_dataset(fp_dataset_val_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:54:45 - /tmp/mistnovelist-eval/eval_1717422885.pkl\n",
      "03-Jun-24 13:54:45 - w-02-0.086-0.079.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for weights_i, weights_ in enumerate(weights_list):\n",
    "    eval_id = str(int(time.time()))\n",
    "    pickle_id = eval_id\n",
    "    if sc.config['eval_id'] != '':\n",
    "        eval_id = sc.config['eval_id']\n",
    "    if sc.config['eval_counter'] != '':\n",
    "        pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "        if len(weights_list) > 1:\n",
    "            pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter'] + \"-\" + weights_i\n",
    "    \n",
    "    # logpath_topn = eval_folder / (\"eval_\" + eval_id + \"_topn.txt\")\n",
    "    # logpath_top1 = eval_folder / (\"eval_\" + eval_id + \"_top1.txt\")\n",
    "    picklepath = eval_folder / (\"eval_\" + pickle_id + \".pkl\")\n",
    "    logger.info(picklepath)\n",
    "    logger.info(weights_)\n",
    "    weights = os.path.join(sc.config[\"weights_folder\"], weights_)\n",
    "\n",
    "    \n",
    "    retain_single_duplicate = True\n",
    "\n",
    "    fp_dataset_iter = iter(fp_dataset_val)\n",
    "    blueprints = gen.dataset_blueprint(fp_dataset_val_)\n",
    "    \n",
    "    # Load models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fingerprint rounding in model\n",
      "using fingerprint rounding in model\n"
     ]
    }
   ],
   "source": [
    "    import model\n",
    "    \n",
    "    model_encode = model.EncoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    model_decode = model.DecoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,)\n",
    "    model_transcode = model.TranscoderModel(\n",
    "                    blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:54:46 - Loading layer encoder weights\n",
      "03-Jun-24 13:54:46 - Loaded\n",
      "03-Jun-24 13:54:46 - Loading layer hydrogen_estimator weights\n",
      "03-Jun-24 13:54:46 - Loaded\n",
      "03-Jun-24 13:54:46 - Loading layer tokens_y weights\n",
      "03-Jun-24 13:54:46 - Loaded\n"
     ]
    }
   ],
   "source": [
    "    # Build models by calling them\n",
    "    y_ = model_transcode(blueprints)\n",
    "    enc = model_encode(next(fp_dataset_iter)[0])\n",
    "    _ = model_decode(enc)\n",
    "    \n",
    "    model_transcode.load_weights(weights, by_name=True)\n",
    "    model_encode.copy_weights(model_transcode)\n",
    "    model_decode.copy_weights(model_transcode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:54:51 - Decoder initialized\n",
      "03-Jun-24 13:54:51 - Processing and scoring predictions\n",
      "03-Jun-24 13:54:51 - Predicting 20 samples - start\n",
      "03-Jun-24 13:54:51 - Beam block size 8*64*128, sequences retrieved per sample: 10\n",
      "100%|██████████| 3/3 [00:06<00:00,  2.27s/it]\n",
      "03-Jun-24 13:54:58 - Predicting 20 samples - done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Initialize decoder\n",
    "    decoder = dec.get_decoder(decoder_name)(\n",
    "        model_encode, model_decode, steps, n, k, kk, config = sc.config)\n",
    "    logger.info(\"Decoder initialized\")\n",
    "    logger.info(f\"Processing and scoring predictions\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger.info(f\"Predicting {n_total} samples - start\")\n",
    "    logger.info(f\"Beam block size {n}*{k}*{steps}, sequences retrieved per sample: {kk}\")\n",
    "    result_blocks = []\n",
    "    reference_blocks = []\n",
    "    for data in tqdm(fp_dataset_val, total = (n_total -1) // n + 1):\n",
    "        # repeat the input data k times for each of n queries\n",
    "        # (now we encode each of k samples individually because the encoding\n",
    "        # may be probabilistic)\n",
    "        \n",
    "        # make a custom decoder if we don't have all n samples\n",
    "        n_real = len(data[0]['n_hydrogen'])\n",
    "        if n_real != n:\n",
    "            decoder = dec.get_decoder(decoder_name)(\n",
    "                    model_encode, model_decode, steps, n_real, k, kk, config = sc.config)\n",
    "        \n",
    "        data_k = {key: tf.repeat(x, k, axis=0) for key, x in data[0].items()}\n",
    "        states_init = model_encode.predict(data_k)\n",
    "        # predict k sequences for each query.\n",
    "        sequences, y, scores = decoder.decode_beam(states_init)\n",
    "        seq, score, length = decoder.beam_traceback(sequences, y, scores)\n",
    "        smiles = decoder.sequence_ytoc(seq)\n",
    "        results_df = decoder.format_results(smiles, score)\n",
    "        result_blocks.append(results_df)\n",
    "        reference_df = decoder.format_reference(\n",
    "            [bytes.decode(x, 'UTF-8') for x in data[1][0].numpy()],\n",
    "            [d for d in data[1][1].numpy()])\n",
    "        reference_blocks.append(reference_df)\n",
    "    results = pd.concat(result_blocks)        \n",
    "    logger.info(f\"Predicting {n_total} samples - done\")\n",
    "    pickle.dump(results, open(\n",
    "        picklepath.with_suffix(\"\").with_name(picklepath.name + \"_all\"), \"wb\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS(=O)(=O)N1CCc2nc(N3CCOCC3)nc(-c3cnc(N)nc3)c21</td>\n",
       "      <td>-5.148147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS(=O)(=O)N1CCOCC1c1nc(N)nc(-c2ccc(N3CC3)nc2)n1</td>\n",
       "      <td>-5.195909</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(N3CCOCC3)nc(-c3cnc(N)nc3)nc21</td>\n",
       "      <td>-5.275101</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS(=O)(=O)c1ccc(-c2nc(N3CCOCC3)nc(N3CCC3)n2)nn1</td>\n",
       "      <td>-5.351298</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(c(-c3cnc(N4CCOCC4)nc3)ncn2)N1</td>\n",
       "      <td>-5.363149</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Oc1ccc(C2Oc3cc(O)cc(O)c3C(c3c(O)cc(O)c4c3OC(c3...</td>\n",
       "      <td>-4.472226</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oc1ccc2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc(O...</td>\n",
       "      <td>-4.547336</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>c1cc(O)c2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc...</td>\n",
       "      <td>-5.021507</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oc1cc(O)cc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4O...</td>\n",
       "      <td>-5.051722</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4OC(c4...</td>\n",
       "      <td>-5.168901</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles     score  id  n  k\n",
       "0     CS(=O)(=O)N1CCc2nc(N3CCOCC3)nc(-c3cnc(N)nc3)c21 -5.148147   0  0  0\n",
       "1     CS(=O)(=O)N1CCOCC1c1nc(N)nc(-c2ccc(N3CC3)nc2)n1 -5.195909   1  0  1\n",
       "2     CS(=O)(=O)N1CCc2c(N3CCOCC3)nc(-c3cnc(N)nc3)nc21 -5.275101   2  0  2\n",
       "3     CS(=O)(=O)c1ccc(-c2nc(N3CCOCC3)nc(N3CCC3)n2)nn1 -5.351298   3  0  3\n",
       "4     CS(=O)(=O)N1CCc2c(c(-c3cnc(N4CCOCC4)nc3)ncn2)N1 -5.363149   4  0  4\n",
       "..                                                ...       ...  .. .. ..\n",
       "35  Oc1ccc(C2Oc3cc(O)cc(O)c3C(c3c(O)cc(O)c4c3OC(c3... -4.472226  35  3  5\n",
       "36  Oc1ccc2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc(O... -4.547336  36  3  6\n",
       "37  c1cc(O)c2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc... -5.021507  37  3  7\n",
       "38  Oc1cc(O)cc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4O... -5.051722  38  3  8\n",
       "39  Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4OC(c4... -5.168901  39  3  9\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:14:00 - Evaluating 20 blocks - start\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/msnovelist/fp_management/fp_database.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"smiles_generic\"] = smiles_generic\n",
      "/msnovelist/fp_management/fp_database.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"smiles_canonical\"] = smiles\n",
      "/msnovelist/fp_management/fp_database.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"mol\"] = mol\n",
      "/msnovelist/fp_management/fp_database.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for m in mol]\n",
      "/msnovelist/fp_management/fp_database.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ik.split('-')[0] for ik in block[\"inchikey\"]\n",
      "/msnovelist/fp_management/fp_database.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for m in mol]\n",
      "/msnovelist/fp_management/fp_database.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"block_id\"] = block_id\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 9 - CS(=O)(=O)Nc1nc(N)nc(-c2ccc(N3CCOCC3)c(N)c2)c1=N\n",
      "failed parsing id 12 - s???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 13 - -=??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 14 - NC1?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 15 - CC12????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 16 - C[NH2+]CC1???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 17 - COC(C(??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 18 - CSCC(C)?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 19 - COC1(C(=????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 20 - OC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C(=O)C(C)C=CC=C(C)C(=O)N1)=C4\n",
      "failed parsing id 22 - COC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C=CC=C(C)C(=O)NC1=O)C4=C=\n",
      "failed parsing id 23 - COC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C=CC=C(C)C(=O)N1)=C4\n",
      "failed parsing id 24 - COC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C=CC=C(C)C(=O)NC1=O)=C4C=C\n",
      "failed parsing id 25 - COC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C=CC=C(C)C(=O)NC1=O)=C4C#C\n",
      "failed parsing id 27 - COC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(OC(C)=O)C(C)C=CC=C(C)C(=O)N1)=C4\n",
      "failed parsing id 28 - OC1C=COC2(C)Oc3c(C)c(O)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C(O)C(C)C=CC=C(C)C(=O)N1)=C4#\n",
      "failed parsing id 32 - s???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 33 - C6??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 34 - CC2?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 35 - CBrC(????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 36 - CC(N)???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 37 - Cc1c(C??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 38 - CC(CCC1?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 39 - CNC(=O)C????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 78 - O=C(OCc1ccc(F)cc1)C1CCCN1Cc1nc(-c2ccccc2)s1\n",
      "failed parsing id 79 - O=C(COCc1ccccc1)N1CCCC1c1nc(-c2ccc(F)cc2)s1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 18 - COc1ccccc1C=C(C#N)C(=O)c1cc(Oc2ccccc2C(=O)O)nn1\n",
      "failed parsing id 34 - CC(=O)OCC1OC(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c2)O1\n",
      "failed parsing id 46 - CC(=O)OCC1OC(OC2OC(COC(C)=O)C(OC(C)=O)C(OC(C)=O)C2OC(C)=O)C(OC(C)=O)C(OC(C)=O)C1OC(=O)c1ccc2ccccc2o1\n",
      "failed parsing id 51 - s????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 52 - SC???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 53 - cC(??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 54 - C(CN?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 55 - CNCC(????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 56 - COC1=C???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 57 - COC=C(C??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 58 - CC1OC(O)?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 59 - CC(=O)NC1????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 69 - CN(C)C(=O)C(CCN1CCC(O)(c2ccccc2)(c2ccc(Cl)cc2)CC1)c1ccccc1\n",
      "failed parsing id 70 - CC1(C)CC(=O)C2=C(C1)N(Cc1cccc(Nc3ncc(CN4CCCC4)cn3)c1)c1cccc1O2\n",
      "failed parsing id 72 - CC1(C)C2CCc3nc(Nc4cccc(O)c4)nc(Nc4ccc(CN5CCCC5)cc4C3=O)C21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 9 - CCOC(=O)C(C)Oc1ccc2nc(COc3ccc(Cl)cc3)nc2c1\n",
      "failed parsing id 33 - Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4OC(c4ccc(O)c(O)c4)C3O)CC(O)C2O)c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    %autoreload\n",
    "    logger.info(f\"Evaluating {n_total} blocks - start\")\n",
    "    \n",
    "    results_evaluated = []\n",
    "    for block_, ref_, block_id in zip(tqdm(result_blocks), \n",
    "                                    reference_blocks,\n",
    "                                    range(len(result_blocks))):\n",
    "        # Make a block with molecule, MF, smiles for candidates and reference\n",
    "        block = db.process_df(block_, fingerprinter,\n",
    "                              construct_from = \"smiles\",\n",
    "                              block_id = block_id)\n",
    "        \n",
    "        if retain_single_duplicate:\n",
    "            block.sort_values(\"score\", ascending = False, inplace = True)\n",
    "            block = block.groupby([\"n\", \"inchikey1\"]).first().reset_index()\n",
    "            \n",
    "        ref = db.process_df(ref_, fingerprinter,\n",
    "                              construct_from = \"smiles\",\n",
    "                              block_id = block_id)\n",
    "        # Also actually compute the true fingerprint for the reference\n",
    "\n",
    "        if sc.config[\"eval_fingerprint_all\"]:\n",
    "            fingerprinter.process_df(ref,\n",
    "                                    out_column = \"fingerprint_ref_true\",\n",
    "                                    inplace=True)\n",
    "            \n",
    "        # Match ref to predictions\n",
    "        block = block.join(ref, on=\"n\", rsuffix=\"_ref\")\n",
    "        # Keep only correct formula\n",
    "        block_ok = block.loc[block[\"inchikey1\"].notna()].loc[block[\"mf\"] == block[\"mf_ref\"]]\n",
    "        # Now actually compute the fingerprints, only for matching MF\n",
    "        if sc.config[\"eval_fingerprint_all\"]:\n",
    "            fingerprinter.process_df(block_ok,\n",
    "                                 inplace=True)\n",
    "        block = block.merge(\n",
    "            block_ok[[\"n\",\"k\",\"fingerprint\"]],\n",
    "            left_on = [\"n\", \"k\"],\n",
    "            right_on = [\"n\", \"k\"],\n",
    "            suffixes = [\"_ref\", \"\"],\n",
    "            how = \"left\")\n",
    "    \n",
    "        results_evaluated.append(block)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:14:16 - Evaluating 20 blocks - merging\n",
      "03-Jun-24 14:14:16 - Pickling predictions from [val]\n"
     ]
    }
   ],
   "source": [
    "    logger.info(f\"Evaluating {n_total} blocks - merging\")\n",
    "    results_complete = pd.concat(results_evaluated)\n",
    "    results_complete[\"nn\"] = n * results_complete[\"block_id\"] + results_complete[\"n\"]\n",
    "    results_complete [\"evaluation_set\"] = evaluation_set\n",
    "    \n",
    "    logger.info(f\"Pickling predictions from [{evaluation_set}]\")\n",
    "    pickle.dump(results_complete, open(picklepath, \"wb\"))\n",
    "    \n",
    "    results_ok = results_complete.loc[results_complete[\"fingerprint\"].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path =  eval_folder / (\"eval_\" + pickle_id + \".tsv\")\n",
    "results_complete.to_csv(out_path, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
