{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 25 15:36:30 2020\n",
    "\n",
    "@author: stravsm\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:21:03 - evaluation startup\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fp_management import database as db\n",
    "from fp_management import mist_fingerprinting as fpr\n",
    "from fp_management import base_fingerprinting as bfpr\n",
    "import smiles_config as sc\n",
    "\n",
    "sc.config_file.append(\"config.EULER-eval.yaml\")\n",
    "sc.config_reload()\n",
    "\n",
    "import infrastructure.generator as gen\n",
    "import infrastructure.decoder as dec\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "import infrastructure.score as msc\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Disable dropout. Is there a more elegant way to adapt config at runtime?\n",
    "sc.config[\"model_config\"][\"training\"] = False\n",
    "\n",
    "# Randomness is relevant for stochastic sampling\n",
    "random_seed = sc.config['random_seed_global']\n",
    "if random_seed != '':\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.experimental.set_seed(random_seed)\n",
    "\n",
    "# Setup logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MSNovelist\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"evaluation startup\")\n",
    "\n",
    "eval_folder = pathlib.Path(sc.config[\"eval_folder\"])\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_id = str(int(time.time()))\n",
    "pickle_id = eval_id\n",
    "if sc.config['eval_id'] != '':\n",
    "    eval_id = sc.config['eval_id']\n",
    "if sc.config['eval_counter'] != '':\n",
    "    pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "    \n",
    "if isinstance(sc.config['weights'], list):\n",
    "    weights_list = sc.config['weights']\n",
    "else:\n",
    "    weights_list = [sc.config['weights']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, do everything independent of weights\n",
    "\n",
    "fpr.MistFingerprinter.init_instance()\n",
    "fingerprinter = fpr.MistFingerprinter.get_instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "n = sc.config[\"eval_n\"]\n",
    "n_total = sc.config[\"eval_n_total\"]\n",
    "#n_total_ = n_total // n * n\n",
    "k = sc.config[\"eval_k\"]\n",
    "kk = sc.config[\"eval_kk\"]\n",
    "steps = sc.config[\"eval_steps\"]\n",
    "\n",
    "decoder_name = sc.config[\"decoder_name\"]\n",
    "\n",
    "evaluation_set = sc.config[\"evaluation_set\"]\n",
    "\n",
    "# File for CSI:FingerID validation data\n",
    "data_eval_ = sc.config[\"db_path_eval\"]\n",
    "# Load mapping table for the CSI:FingerID predictors\n",
    "# Load dataset and process appropriately\n",
    "db_eval = db.FpDatabase.load_from_config(data_eval_)\n",
    "pipeline_options =  db_eval.get_pipeline_options()\n",
    "    \n",
    "pipeline_encoder = sc.config['pipeline_encoder']\n",
    "pipeline_reference = sc.config['pipeline_reference']\n",
    "\n",
    "dataset_val = db_eval.get_grp(evaluation_set)\n",
    "if n_total != -1:\n",
    "    dataset_val = dataset_val[:n_total]\n",
    "else:\n",
    "    n_total = len(dataset_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_X': False,\n",
       " 'unpackbits': False,\n",
       " 'unpack': False,\n",
       " 'fingerprint_selected': 'fingerprint_degraded'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-the-fly translate the dataset :(\n",
    "# \n",
    "\n",
    "def entry_for_row(row):\n",
    "    res = {\n",
    "        key: row[key] for key in row.keys()\n",
    "    }\n",
    "    res[\"fingerprint\"] = fingerprinter.get_fp(row[\"fingerprint\"])[0,:]\n",
    "    res[\"fingerprint_degraded\"] = fingerprinter.get_fp(row[\"fingerprint_degraded\"])[0,:]\n",
    "    return res\n",
    "\n",
    "dataset_val_mapped = [entry_for_row(x) for x in dataset_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dataset_val_mapped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"fingerprint_degraded\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:45:20 - using unpickle_mf\n",
      "03-Jun-24 13:45:20 - not using unpack\n",
      "03-Jun-24 13:45:20 - not using fp_map\n",
      "03-Jun-24 13:45:21 - not using embed_X\n",
      "03-Jun-24 13:45:21 - Selecting fingerprint fingerprint_degraded\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Load dataset and sampler, apply sampler to dataset\n",
    "# (so we can also evaluate from fingerprint_sampled)\n",
    "fp_dataset_val_ = gen.smiles_pipeline(dataset_val_mapped,\n",
    "                                    batch_size = n,\n",
    "                                    **pipeline_options,\n",
    "                                    map_fingerprints=False,\n",
    "                                    degraded_fingerprint_type = \"uint8\")\n",
    "\n",
    "fp_dataset_val = gen.dataset_zip(fp_dataset_val_, \n",
    "                                 pipeline_encoder, pipeline_reference,\n",
    "                                 **pipeline_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fingerprint_selected': <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       "  'mol_form': <tf.Tensor: shape=(8, 10), dtype=float32, numpy=\n",
       "  array([[15.,  0.,  0.,  0.,  7.,  3.,  0.,  0.,  1., 19.],\n",
       "         [53.,  0.,  0.,  0.,  0., 22.,  0.,  0.,  0., 90.],\n",
       "         [47.,  0.,  0.,  0.,  2., 13.,  0.,  0.,  0., 56.],\n",
       "         [54.,  0.,  0.,  0.,  0., 23.,  0.,  0.,  0., 92.],\n",
       "         [16.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  1., 16.],\n",
       "         [21.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  0., 28.],\n",
       "         [23.,  0.,  0.,  2.,  7.,  1.,  0.,  0.,  0., 31.],\n",
       "         [21.,  1.,  0.,  0.,  2.,  2.,  0.,  0.,  1., 19.]], dtype=float32)>,\n",
       "  'n_hydrogen': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([19., 90., 56., 92., 16., 28., 31., 19.], dtype=float32)>},\n",
       " (<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
       "  array([b'CS(=O)(=O)N1CCc2c(-c3cnc(N)nc3)nc(N3CCOCC3)nc21',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OCC(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COC1C=COC2(C)Oc3c(C)c(OC(=O)c4ccc(C)cc4)c4c(c3C2=O)C(=O)C(N(C)C)=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(OC(C)=O)C1C)C4=O',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OC(CO)C(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COc1ccc(C2CC(c3ccsc3)=NN2C(C)=O)cc1O',\n",
       "         b'CCOc1ccc(N2C(=O)CC(N3CCCC4CCCCC43)C2=O)cc1',\n",
       "         b'CCC1CN(c2nc(N)c(C(N)=O)nc2Cl)CCN1C1CCN(Cc2ccc(Cl)cc2)CC1',\n",
       "         b'O=C(OCc1ccccc1)C1CCCN1c1nc(-c2ccc(F)cc2)cs1'], dtype=object)>,\n",
       "  <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(fp_dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:45:29 - Sampler basic_tp_fp loading\n",
      "03-Jun-24 13:45:29 - Sampler basic_tp_fp loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampler_name = sc.config['sampler_name']\n",
    "round_fingerprints = True\n",
    "if sampler_name != '':\n",
    "    logger.info(f\"Sampler {sampler_name} loading\")\n",
    "    sampler_module = importlib.import_module('fp_sampling.' + sampler_name, 'fp_sampling')\n",
    "    sampler_factory = sampler_module.SamplerFactory(sc.config)\n",
    "    round_fingerprints = sampler_factory.round_fingerprint_inference()\n",
    "    sampler = sampler_factory.get_sampler()\n",
    "    logger.info(f\"Sampler {sampler_name} loaded\")\n",
    "    fp_dataset_val_ = sampler.map_dataset(fp_dataset_val_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:45:30 - /tmp/mistnovelist-eval/eval_1717422330.pkl\n",
      "03-Jun-24 13:45:30 - w-02-0.086-0.079.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for weights_i, weights_ in enumerate(weights_list):\n",
    "    eval_id = str(int(time.time()))\n",
    "    pickle_id = eval_id\n",
    "    if sc.config['eval_id'] != '':\n",
    "        eval_id = sc.config['eval_id']\n",
    "    if sc.config['eval_counter'] != '':\n",
    "        pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "        if len(weights_list) > 1:\n",
    "            pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter'] + \"-\" + weights_i\n",
    "    \n",
    "    # logpath_topn = eval_folder / (\"eval_\" + eval_id + \"_topn.txt\")\n",
    "    # logpath_top1 = eval_folder / (\"eval_\" + eval_id + \"_top1.txt\")\n",
    "    picklepath = eval_folder / (\"eval_\" + pickle_id + \".pkl\")\n",
    "    logger.info(picklepath)\n",
    "    logger.info(weights_)\n",
    "    weights = os.path.join(sc.config[\"weights_folder\"], weights_)\n",
    "\n",
    "    \n",
    "    retain_single_duplicate = True\n",
    "\n",
    "    fp_dataset_iter = iter(fp_dataset_val)\n",
    "    blueprints = gen.dataset_blueprint(fp_dataset_val_)\n",
    "    \n",
    "    # Load models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fingerprint rounding in model\n",
      "using fingerprint rounding in model\n"
     ]
    }
   ],
   "source": [
    "    import model\n",
    "    \n",
    "    model_encode = model.EncoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    model_decode = model.DecoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,)\n",
    "    model_transcode = model.TranscoderModel(\n",
    "                    blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 13:45:37 - Loading layer encoder weights\n",
      "03-Jun-24 13:45:37 - Loaded\n",
      "03-Jun-24 13:45:37 - Loading layer hydrogen_estimator weights\n",
      "03-Jun-24 13:45:37 - Loaded\n",
      "03-Jun-24 13:45:37 - Loading layer tokens_y weights\n",
      "03-Jun-24 13:45:37 - Loaded\n"
     ]
    }
   ],
   "source": [
    "    # Build models by calling them\n",
    "    y_ = model_transcode(blueprints)\n",
    "    enc = model_encode(next(fp_dataset_iter)[0])\n",
    "    _ = model_decode(enc)\n",
    "    \n",
    "    model_transcode.load_weights(weights, by_name=True)\n",
    "    model_encode.copy_weights(model_transcode)\n",
    "    model_decode.copy_weights(model_transcode)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
