{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 25 15:36:30 2020\n",
    "\n",
    "@author: stravsm\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fp_management import database as db\n",
    "from fp_management import mist_fingerprinting\n",
    "import smiles_config as sc\n",
    "\n",
    "sc.config_file.append(\"config.EULER-eval.yaml\")\n",
    "sc.config_reload()\n",
    "\n",
    "import infrastructure.generator as gen\n",
    "import infrastructure.decoder as dec\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "import infrastructure.score as msc\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Disable dropout. Is there a more elegant way to adapt config at runtime?\n",
    "sc.config[\"model_config\"][\"training\"] = False\n",
    "\n",
    "# Randomness is relevant for stochastic sampling\n",
    "random_seed = sc.config['random_seed_global']\n",
    "if random_seed != '':\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.experimental.set_seed(random_seed)\n",
    "\n",
    "# Setup logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MSNovelist\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"evaluation startup\")\n",
    "\n",
    "eval_folder = pathlib.Path(sc.config[\"eval_folder\"])\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_id = str(int(time.time()))\n",
    "pickle_id = eval_id\n",
    "if sc.config['eval_id'] != '':\n",
    "    eval_id = sc.config['eval_id']\n",
    "if sc.config['eval_counter'] != '':\n",
    "    pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "    \n",
    "if isinstance(sc.config['weights'], list):\n",
    "    weights_list = sc.config['weights']\n",
    "else:\n",
    "    weights_list = [sc.config['weights']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, do everything independent of weights\n",
    "\n",
    "fpr.MistFingerprinter.init_instance()\n",
    "fingerprinter = fpr.Fingerprinter.get_instance()\n",
    "\n",
    "  \n",
    "n = sc.config[\"eval_n\"]\n",
    "n_total = sc.config[\"eval_n_total\"]\n",
    "#n_total_ = n_total // n * n\n",
    "k = sc.config[\"eval_k\"]\n",
    "kk = sc.config[\"eval_kk\"]\n",
    "steps = sc.config[\"eval_steps\"]\n",
    "\n",
    "decoder_name = sc.config[\"decoder_name\"]\n",
    "\n",
    "sc.config.setdefault('cv_fold', 0)\n",
    "cv_fold = sc.config[\"cv_fold\"]\n",
    "#evaluation_set_ = sc.config['evaluation_set']\n",
    "evaluation_set = f\"fold{cv_fold}\"\n",
    "\n",
    "# File for CSI:FingerID validation data\n",
    "data_eval_ = sc.config[\"db_path_eval\"]\n",
    "# Load mapping table for the CSI:FingerID predictors\n",
    "# Load dataset and process appropriately\n",
    "db_eval = db.FpDatabase.load_from_config(data_eval_)\n",
    "pipeline_options =  db_eval.get_pipeline_options()\n",
    "    \n",
    "pipeline_encoder = sc.config['pipeline_encoder']\n",
    "pipeline_reference = sc.config['pipeline_reference']\n",
    "\n",
    "dataset_val = db_eval.get_grp(evaluation_set)\n",
    "if n_total != -1:\n",
    "    dataset_val = dataset_val[:n_total]\n",
    "else:\n",
    "    n_total = len(dataset_val)\n",
    "\n",
    "# Load dataset and sampler, apply sampler to dataset\n",
    "# (so we can also evaluate from fingerprint_sampled)\n",
    "fp_dataset_val_ = gen.smiles_pipeline(dataset_val,\n",
    "                                    batch_size = n,\n",
    "                                    **pipeline_options,\n",
    "                                    map_fingerprints=False)\n",
    "\n",
    "fp_dataset_val = gen.dataset_zip(fp_dataset_val_, \n",
    "                                 pipeline_encoder, pipeline_reference,\n",
    "                                 **pipeline_options)\n",
    "\n",
    "sampler_name = sc.config['sampler_name']\n",
    "round_fingerprints = True\n",
    "if sampler_name != '':\n",
    "    logger.info(f\"Sampler {sampler_name} loading\")\n",
    "    sampler_module = importlib.import_module('fp_sampling.' + sampler_name, 'fp_sampling')\n",
    "    sampler_factory = sampler_module.SamplerFactory(sc.config)\n",
    "    round_fingerprints = sampler_factory.round_fingerprint_inference()\n",
    "    sampler = sampler_factory.get_sampler()\n",
    "    logger.info(f\"Sampler {sampler_name} loaded\")\n",
    "    fp_dataset_val_ = sampler.map_dataset(fp_dataset_val_)\n",
    "\n",
    "\n",
    "for weights_i, weights_ in enumerate(weights_list):\n",
    "    eval_id = str(int(time.time()))\n",
    "    pickle_id = eval_id\n",
    "    if sc.config['eval_id'] != '':\n",
    "        eval_id = sc.config['eval_id']\n",
    "    if sc.config['eval_counter'] != '':\n",
    "        pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "        if len(weights_list) > 1:\n",
    "            pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter'] + \"-\" + weights_i\n",
    "    \n",
    "    # logpath_topn = eval_folder / (\"eval_\" + eval_id + \"_topn.txt\")\n",
    "    # logpath_top1 = eval_folder / (\"eval_\" + eval_id + \"_top1.txt\")\n",
    "    picklepath = eval_folder / (\"eval_\" + pickle_id + \".pkl\")\n",
    "    logger.info(picklepath)\n",
    "    logger.info(weights_)\n",
    "    weights = os.path.join(sc.config[\"weights_folder\"], weights_)\n",
    "\n",
    "    \n",
    "    retain_single_duplicate = True\n",
    "\n",
    "    fp_dataset_iter = iter(fp_dataset_val)\n",
    "    blueprints = gen.dataset_blueprint(fp_dataset_val_)\n",
    "    \n",
    "    # Load models\n",
    "    \n",
    "    import model\n",
    "    \n",
    "    model_encode = model.EncoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    model_decode = model.DecoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,)\n",
    "    model_transcode = model.TranscoderModel(\n",
    "                    blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    \n",
    "    # Build models by calling them\n",
    "    y_ = model_transcode(blueprints)\n",
    "    enc = model_encode(next(fp_dataset_iter)[0])\n",
    "    _ = model_decode(enc)\n",
    "    \n",
    "    model_transcode.load_weights(weights, by_name=True)\n",
    "    model_encode.copy_weights(model_transcode)\n",
    "    model_decode.copy_weights(model_transcode)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
