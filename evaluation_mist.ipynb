{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 25 15:36:30 2020\n",
    "\n",
    "@author: stravsm\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:31:58 - evaluation startup\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fp_management import database as db\n",
    "from fp_management import mist_fingerprinting as fpr\n",
    "from fp_management import fingerprint_map as fpm\n",
    "import smiles_config as sc\n",
    "\n",
    "sc.config_file.append(\"config.EULER-eval.yaml\")\n",
    "sc.config_reload()\n",
    "\n",
    "import infrastructure.generator as gen\n",
    "import infrastructure.decoder as dec\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "import infrastructure.score as msc\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Disable dropout. Is there a more elegant way to adapt config at runtime?\n",
    "sc.config[\"model_config\"][\"training\"] = False\n",
    "\n",
    "# Randomness is relevant for stochastic sampling\n",
    "random_seed = sc.config['random_seed_global']\n",
    "if random_seed != '':\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.experimental.set_seed(random_seed)\n",
    "\n",
    "# Setup logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MSNovelist\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"evaluation startup\")\n",
    "\n",
    "eval_folder = pathlib.Path(sc.config[\"eval_folder\"])\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_id = str(int(time.time()))\n",
    "pickle_id = eval_id\n",
    "if sc.config['eval_id'] != '':\n",
    "    eval_id = sc.config['eval_id']\n",
    "if sc.config['eval_counter'] != '':\n",
    "    pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "    \n",
    "if isinstance(sc.config['weights'], list):\n",
    "    weights_list = sc.config['weights']\n",
    "else:\n",
    "    weights_list = [sc.config['weights']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, do everything independent of weights\n",
    "\n",
    "fpr.MistFingerprinter.init_instance()\n",
    "fingerprinter = fpr.MistFingerprinter.get_instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "n = sc.config[\"eval_n\"]\n",
    "n_total = sc.config[\"eval_n_total\"]\n",
    "#n_total_ = n_total // n * n\n",
    "k = sc.config[\"eval_k\"]\n",
    "kk = sc.config[\"eval_kk\"]\n",
    "steps = sc.config[\"eval_steps\"]\n",
    "\n",
    "decoder_name = sc.config[\"decoder_name\"]\n",
    "\n",
    "evaluation_set = sc.config[\"evaluation_set\"]\n",
    "\n",
    "# File for CSI:FingerID validation data\n",
    "data_eval_ = sc.config[\"db_path_eval\"]\n",
    "# Load mapping table for the CSI:FingerID predictors\n",
    "# Load dataset and process appropriately\n",
    "db_eval = db.FpDatabase.load_from_config(data_eval_)\n",
    "pipeline_options =  db_eval.get_pipeline_options()\n",
    "    \n",
    "pipeline_encoder = sc.config['pipeline_encoder']\n",
    "pipeline_reference = sc.config['pipeline_reference']\n",
    "\n",
    "dataset_val = db_eval.get_grp(evaluation_set)\n",
    "if n_total != -1:\n",
    "    dataset_val = dataset_val[:n_total]\n",
    "else:\n",
    "    n_total = len(dataset_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_X': False,\n",
       " 'unpackbits': False,\n",
       " 'unpack': False,\n",
       " 'fingerprint_selected': 'fingerprint_degraded'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-the-fly translate the dataset :(\n",
    "# \n",
    "\n",
    "def entry_for_row(row):\n",
    "    res = {\n",
    "        key: row[key] for key in row.keys()\n",
    "    }\n",
    "    res[\"fingerprint\"] = fingerprinter.get_fp(row[\"fingerprint\"])[0,:]\n",
    "    res[\"fingerprint_degraded\"] = fingerprinter.get_fp(row[\"fingerprint_degraded\"])[0,:]\n",
    "    return res\n",
    "\n",
    "dataset_val_mapped = [entry_for_row(x) for x in dataset_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dataset_val_mapped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"fingerprint_degraded\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:05 - using unpickle_mf\n",
      "03-Jun-24 14:28:05 - not using unpack\n",
      "03-Jun-24 14:28:05 - not using fp_map\n",
      "03-Jun-24 14:28:06 - not using embed_X\n",
      "03-Jun-24 14:28:06 - Selecting fingerprint fingerprint_degraded\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# Load dataset and sampler, apply sampler to dataset\n",
    "# (so we can also evaluate from fingerprint_sampled)\n",
    "fp_dataset_val_ = gen.smiles_pipeline(dataset_val_mapped,\n",
    "                                    batch_size = n,\n",
    "                                    **pipeline_options,\n",
    "                                    map_fingerprints=False,\n",
    "                                    degraded_fingerprint_type = \"uint8\")\n",
    "\n",
    "fp_dataset_val = gen.dataset_zip(fp_dataset_val_, \n",
    "                                 pipeline_encoder, pipeline_reference,\n",
    "                                 **pipeline_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fingerprint_selected': <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       "  'mol_form': <tf.Tensor: shape=(8, 10), dtype=float32, numpy=\n",
       "  array([[15.,  0.,  0.,  0.,  7.,  3.,  0.,  0.,  1., 19.],\n",
       "         [53.,  0.,  0.,  0.,  0., 22.,  0.,  0.,  0., 90.],\n",
       "         [47.,  0.,  0.,  0.,  2., 13.,  0.,  0.,  0., 56.],\n",
       "         [54.,  0.,  0.,  0.,  0., 23.,  0.,  0.,  0., 92.],\n",
       "         [16.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  1., 16.],\n",
       "         [21.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  0., 28.],\n",
       "         [23.,  0.,  0.,  2.,  7.,  1.,  0.,  0.,  0., 31.],\n",
       "         [21.,  1.,  0.,  0.,  2.,  2.,  0.,  0.,  1., 19.]], dtype=float32)>,\n",
       "  'n_hydrogen': <tf.Tensor: shape=(8,), dtype=float32, numpy=array([19., 90., 56., 92., 16., 28., 31., 19.], dtype=float32)>},\n",
       " (<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
       "  array([b'CS(=O)(=O)N1CCc2c(-c3cnc(N)nc3)nc(N3CCOCC3)nc21',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OCC(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COC1C=COC2(C)Oc3c(C)c(OC(=O)c4ccc(C)cc4)c4c(c3C2=O)C(=O)C(N(C)C)=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(OC(C)=O)C1C)C4=O',\n",
       "         b'CC(C)=CCCC(C)(OC1OC(COC2OC(CO)C(O)C(O)C2O)C(O)C(O)C1O)C1CCC2(C)C1C(O)CC1C3(C)CCC(OC4OC(CO)C(O)C(O)C4OC4OC(CO)C(O)C(O)C4O)C(C)(C)C3CCC12C',\n",
       "         b'COc1ccc(C2CC(c3ccsc3)=NN2C(C)=O)cc1O',\n",
       "         b'CCOc1ccc(N2C(=O)CC(N3CCCC4CCCCC43)C2=O)cc1',\n",
       "         b'CCC1CN(c2nc(N)c(C(N)=O)nc2Cl)CCN1C1CCN(Cc2ccc(Cl)cc2)CC1',\n",
       "         b'O=C(OCc1ccccc1)C1CCCN1c1nc(-c2ccc(F)cc2)cs1'], dtype=object)>,\n",
       "  <tf.Tensor: shape=(8, 4096), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(fp_dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:06 - Sampler basic_tp_fp loading\n",
      "03-Jun-24 14:28:07 - Sampler basic_tp_fp loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampler_name = sc.config['sampler_name']\n",
    "round_fingerprints = True\n",
    "if sampler_name != '':\n",
    "    logger.info(f\"Sampler {sampler_name} loading\")\n",
    "    sampler_module = importlib.import_module('fp_sampling.' + sampler_name, 'fp_sampling')\n",
    "    sampler_factory = sampler_module.SamplerFactory(sc.config)\n",
    "    round_fingerprints = sampler_factory.round_fingerprint_inference()\n",
    "    sampler = sampler_factory.get_sampler()\n",
    "    logger.info(f\"Sampler {sampler_name} loaded\")\n",
    "    fp_dataset_val_ = sampler.map_dataset(fp_dataset_val_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:07 - /data/MSNovelist-results/eval_1717424887.pkl\n",
      "03-Jun-24 14:28:07 - w-05-0.071-0.069.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for weights_i, weights_ in enumerate(weights_list):\n",
    "    eval_id = str(int(time.time()))\n",
    "    pickle_id = eval_id\n",
    "    if sc.config['eval_id'] != '':\n",
    "        eval_id = sc.config['eval_id']\n",
    "    if sc.config['eval_counter'] != '':\n",
    "        pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter']\n",
    "        if len(weights_list) > 1:\n",
    "            pickle_id = sc.config['eval_id'] + \"-\" + sc.config['eval_counter'] + \"-\" + weights_i\n",
    "    \n",
    "    # logpath_topn = eval_folder / (\"eval_\" + eval_id + \"_topn.txt\")\n",
    "    # logpath_top1 = eval_folder / (\"eval_\" + eval_id + \"_top1.txt\")\n",
    "    picklepath = eval_folder / (\"eval_\" + pickle_id + \".pkl\")\n",
    "    logger.info(picklepath)\n",
    "    logger.info(weights_)\n",
    "    weights = os.path.join(sc.config[\"weights_folder\"], weights_)\n",
    "\n",
    "    \n",
    "    retain_single_duplicate = True\n",
    "\n",
    "    fp_dataset_iter = iter(fp_dataset_val)\n",
    "    blueprints = gen.dataset_blueprint(fp_dataset_val_)\n",
    "    \n",
    "    # Load models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fingerprint rounding in model\n",
      "using fingerprint rounding in model\n"
     ]
    }
   ],
   "source": [
    "    import model\n",
    "    \n",
    "    model_encode = model.EncoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    model_decode = model.DecoderModel(\n",
    "                     blueprints = blueprints,\n",
    "                     config = sc.config,)\n",
    "    model_transcode = model.TranscoderModel(\n",
    "                    blueprints = blueprints,\n",
    "                     config = sc.config,\n",
    "                     round_fingerprints = round_fingerprints)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ({fingerprint_selected: (None, 4096), mol_form: (None, 10), n_hydrogen: (None,)}, ((None,), (None, 4096))), types: ({fingerprint_selected: tf.float32, mol_form: tf.float32, n_hydrogen: tf.float32}, (tf.string, tf.float32))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:09 - Loading layer encoder weights\n",
      "03-Jun-24 14:28:09 - Loaded\n",
      "03-Jun-24 14:28:09 - Loading layer hydrogen_estimator weights\n",
      "03-Jun-24 14:28:09 - Loaded\n",
      "03-Jun-24 14:28:09 - Loading layer tokens_y weights\n",
      "03-Jun-24 14:28:09 - Loaded\n"
     ]
    }
   ],
   "source": [
    "    # Build models by calling them\n",
    "    y_ = model_transcode(blueprints)\n",
    "    enc = model_encode(next(fp_dataset_iter)[0])\n",
    "    _ = model_decode(enc)\n",
    "    \n",
    "    model_transcode.load_weights(weights, by_name=True)\n",
    "    model_encode.copy_weights(model_transcode)\n",
    "    model_decode.copy_weights(model_transcode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:09 - Decoder initialized\n",
      "03-Jun-24 14:28:09 - Processing and scoring predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Initialize decoder\n",
    "    decoder = dec.get_decoder(decoder_name)(\n",
    "        model_encode, model_decode, steps, n, k, kk, config = sc.config)\n",
    "    logger.info(\"Decoder initialized\")\n",
    "    logger.info(f\"Processing and scoring predictions\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:09 - Predicting 20 samples - start\n",
      "03-Jun-24 14:28:09 - Beam block size 8*64*128, sequences retrieved per sample: 10\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.46s/it]\n",
      "03-Jun-24 14:28:17 - Predicting 20 samples - done\n"
     ]
    }
   ],
   "source": [
    "    logger.info(f\"Predicting {n_total} samples - start\")\n",
    "    logger.info(f\"Beam block size {n}*{k}*{steps}, sequences retrieved per sample: {kk}\")\n",
    "    result_blocks = []\n",
    "    reference_blocks = []\n",
    "    for data in tqdm(fp_dataset_val, total = (n_total -1) // n + 1):\n",
    "        # repeat the input data k times for each of n queries\n",
    "        # (now we encode each of k samples individually because the encoding\n",
    "        # may be probabilistic)\n",
    "        \n",
    "        # make a custom decoder if we don't have all n samples\n",
    "        n_real = len(data[0]['n_hydrogen'])\n",
    "        if n_real != n:\n",
    "            decoder = dec.get_decoder(decoder_name)(\n",
    "                    model_encode, model_decode, steps, n_real, k, kk, config = sc.config)\n",
    "        \n",
    "        data_k = {key: tf.repeat(x, k, axis=0) for key, x in data[0].items()}\n",
    "        states_init = model_encode.predict(data_k)\n",
    "        # predict k sequences for each query.\n",
    "        sequences, y, scores = decoder.decode_beam(states_init)\n",
    "        seq, score, length = decoder.beam_traceback(sequences, y, scores)\n",
    "        smiles = decoder.sequence_ytoc(seq)\n",
    "        results_df = decoder.format_results(smiles, score)\n",
    "        result_blocks.append(results_df)\n",
    "        reference_df = decoder.format_reference(\n",
    "            [bytes.decode(x, 'UTF-8') for x in data[1][0].numpy()],\n",
    "            [d for d in data[1][1].numpy()])\n",
    "        reference_blocks.append(reference_df)\n",
    "    results = pd.concat(result_blocks)        \n",
    "    logger.info(f\"Predicting {n_total} samples - done\")\n",
    "    pickle.dump(results, open(\n",
    "        picklepath.with_suffix(\"\").with_name(picklepath.name + \"_all\"), \"wb\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS(=O)(=O)N1CCc2nc(-c3cnc(N)nc3N3CCOCC3)ncc21</td>\n",
       "      <td>-3.732617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(N)nc(-c3cnc(N)nc3N3CCOC3)cc21</td>\n",
       "      <td>-3.855933</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(N3CCOCC3)nc(-c3cnc(N)nc3)nc21</td>\n",
       "      <td>-3.999996</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(N)nc(-c3cnc(N4CCOCC4)nc3)nc21</td>\n",
       "      <td>-4.053775</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS(=O)(=O)N1CCc2c(-c3cnc(N)nc3)nc(N3CCOCC3)nc21</td>\n",
       "      <td>-4.145812</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>c1cc(O)c2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc...</td>\n",
       "      <td>-4.734622</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>c1c2c(O)cc(O)c(C3c4c(O)cc(O)cc4OC(c4ccc(O)c(O)...</td>\n",
       "      <td>-4.850604</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Oc1cc(O)c(C2c3c(O)cc(O)cc3OC(c3ccc(O)c(O)c3)C2...</td>\n",
       "      <td>-4.935221</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4OC(c4...</td>\n",
       "      <td>-5.153457</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4ccc(O)cc4OC(c4ccc...</td>\n",
       "      <td>-5.285930</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles     score  id  n  k\n",
       "0       CS(=O)(=O)N1CCc2nc(-c3cnc(N)nc3N3CCOCC3)ncc21 -3.732617   0  0  0\n",
       "1     CS(=O)(=O)N1CCc2c(N)nc(-c3cnc(N)nc3N3CCOC3)cc21 -3.855933   1  0  1\n",
       "2     CS(=O)(=O)N1CCc2c(N3CCOCC3)nc(-c3cnc(N)nc3)nc21 -3.999996   2  0  2\n",
       "3     CS(=O)(=O)N1CCc2c(N)nc(-c3cnc(N4CCOCC4)nc3)nc21 -4.053775   3  0  3\n",
       "4     CS(=O)(=O)N1CCc2c(-c3cnc(N)nc3)nc(N3CCOCC3)nc21 -4.145812   4  0  4\n",
       "..                                                ...       ...  .. .. ..\n",
       "35  c1cc(O)c2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2c1c(O)cc... -4.734622  35  3  5\n",
       "36  c1c2c(O)cc(O)c(C3c4c(O)cc(O)cc4OC(c4ccc(O)c(O)... -4.850604  36  3  6\n",
       "37  Oc1cc(O)c(C2c3c(O)cc(O)cc3OC(c3ccc(O)c(O)c3)C2... -4.935221  37  3  7\n",
       "38  Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4c(O)cc(O)cc4OC(c4... -5.153457  38  3  8\n",
       "39  Oc1ccc(C2Oc3c(c(O)cc(O)c3C3c4ccc(O)cc4OC(c4ccc... -5.285930  39  3  9\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:18 - Evaluating 20 blocks - start\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/msnovelist/fp_management/fp_database.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"smiles_generic\"] = smiles_generic\n",
      "/msnovelist/fp_management/fp_database.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"smiles_canonical\"] = smiles\n",
      "/msnovelist/fp_management/fp_database.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"mol\"] = mol\n",
      "/msnovelist/fp_management/fp_database.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for m in mol]\n",
      "/msnovelist/fp_management/fp_database.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ik.split('-')[0] for ik in block[\"inchikey\"]\n",
      "/msnovelist/fp_management/fp_database.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for m in mol]\n",
      "/msnovelist/fp_management/fp_database.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  block[\"block_id\"] = block_id\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 12 - s???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 13 - c=??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 14 - C?O?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 15 - CC(P????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 16 - CNCCC???????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 17 - CC(C=C??????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 18 - COCC(=O?????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 19 - CC(=CBr)C????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 22 - COC1C=COC2(C)Oc3c(C)c(O)c4c(O)c(c(C=NOC(=O)c5ccc(C)cc5)c4c3C2=O)NC(=O)C(C)=CC=CC(C)C(O)C(C)C(O)C(C)C(OC(C)=O)C(C)C1C\n",
      "failed parsing id 29 - OC1C=COC2(C)Oc3c(C)c(OC(=O)c4ccc(N(C)C)cc4)c4c(c3C2=O)C(=O)C=C(NC(=O)C(C)=CC=CC(C)C(O)C(C)C(OC(C)=O)C(C)C(OC(C)=O)C1C)1\n",
      "failed parsing id 32 - s???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 33 - C-??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 34 - ??(?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 35 - CCP(????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 36 - CP(=O???????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 37 - COC(OC??????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 38 - CC(=CC=?????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 39 - COCOC1C(????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 77 - O=C(OCc1csc(-c2ccc(F)cc2)n1)C1CCCN1Cc1cccc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 12 - COc1ccccc1C=C(C#N)C(=O)c1ccc(Oc2ncncc2C(=O)O)c1\n",
      "failed parsing id 19 - COc1ccccc1C=C(C#N)C(=O)c1ccc(Oc2ncnc(C(=O)O)c2)c1\n",
      "failed parsing id 33 - O=C(OCC1OC(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2ccc(O)c2)C1O)O\n",
      "failed parsing id 35 - O=C(OCC1OC(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c2)C1)O\n",
      "failed parsing id 36 - O=C(OCC1OC(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c(O)c2)C(O)C1OC(=O)c1cccc1)OO\n",
      "failed parsing id 38 - CC(=O)OCC1OC(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c(O)c2)C(OC(=O)c2cc(O)c(O)c2)O1\n",
      "failed parsing id 48 - CC(=O)OCC1OC(OC2C(OC(C)=O)C(OC(C)=O)C(OC(C)=O)C(OC(C)=O)C(OC(C)=O)C2OC(C)=O)C(OC(C)=O)C(=O)C1=CC(=O)c1cccc1\n",
      "failed parsing id 50 - c\n",
      "failed parsing id 53 - s????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 54 - 7C???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 55 - CC[N+]??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 56 - [C-]CC1?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 57 - CC(CO????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 58 - CNC1=C???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "failed parsing id 59 - CC(C)OC??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parsing id 36 - c1c2c(O)cc(O)c(C3c4c(O)cc(O)cc4OC(c4ccc(O)c(O)c4)C3O)c2OC(c2ccc(O)c(O)c2)C1O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    %autoreload\n",
    "    logger.info(f\"Evaluating {n_total} blocks - start\")\n",
    "    \n",
    "    results_evaluated = []\n",
    "    for block_, ref_, block_id in zip(tqdm(result_blocks), \n",
    "                                    reference_blocks,\n",
    "                                    range(len(result_blocks))):\n",
    "        # Make a block with molecule, MF, smiles for candidates and reference\n",
    "        block = db.process_df(block_, fingerprinter,\n",
    "                              construct_from = \"smiles\",\n",
    "                              block_id = block_id)\n",
    "        \n",
    "        if retain_single_duplicate:\n",
    "            block.sort_values(\"score\", ascending = False, inplace = True)\n",
    "            block = block.groupby([\"n\", \"inchikey1\"]).first().reset_index()\n",
    "            \n",
    "        ref = db.process_df(ref_, fingerprinter,\n",
    "                              construct_from = \"smiles\",\n",
    "                              block_id = block_id)\n",
    "        # Also actually compute the true fingerprint for the reference\n",
    "\n",
    "        if sc.config[\"eval_fingerprint_all\"]:\n",
    "            fingerprinter.process_df(ref,\n",
    "                                    out_column = \"fingerprint_ref_true\",\n",
    "                                    inplace=True)\n",
    "            \n",
    "        # Match ref to predictions\n",
    "        block = block.join(ref, on=\"n\", rsuffix=\"_ref\")\n",
    "        # Keep only correct formula\n",
    "        block_ok = block.loc[block[\"inchikey1\"].notna()].loc[block[\"mf\"] == block[\"mf_ref\"]]\n",
    "        # Now actually compute the fingerprints, only for matching MF\n",
    "        if sc.config[\"eval_fingerprint_all\"]:\n",
    "            fingerprinter.process_df(block_ok,\n",
    "                                 inplace=True)\n",
    "        block = block.merge(\n",
    "            block_ok[[\"n\",\"k\",\"fingerprint\"]],\n",
    "            left_on = [\"n\", \"k\"],\n",
    "            right_on = [\"n\", \"k\"],\n",
    "            suffixes = [\"_ref\", \"\"],\n",
    "            how = \"left\")\n",
    "    \n",
    "        results_evaluated.append(block)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jun-24 14:28:18 - Evaluating 20 blocks - merging\n",
      "03-Jun-24 14:28:18 - Pickling predictions from [val]\n"
     ]
    }
   ],
   "source": [
    "    logger.info(f\"Evaluating {n_total} blocks - merging\")\n",
    "    results_complete = pd.concat(results_evaluated)\n",
    "    results_complete[\"nn\"] = n * results_complete[\"block_id\"] + results_complete[\"n\"]\n",
    "    results_complete [\"evaluation_set\"] = evaluation_set\n",
    "    \n",
    "    logger.info(f\"Pickling predictions from [{evaluation_set}]\")\n",
    "    pickle.dump(results_complete, open(picklepath, \"wb\"))\n",
    "    \n",
    "    results_ok = results_complete.loc[results_complete[\"fingerprint\"].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_complete.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_folder = pathlib.Path(\"/data/MSNovelist-results\")\n",
    "out_path =  eval_folder / (\"eval_\" + pickle_id + \".tsv\")\n",
    "\n",
    "\n",
    "results_complete.to_csv(out_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_out_path =  eval_folder / (\"eval_\" + pickle_id + \"_short.tsv\")\n",
    "results_complete[[\"smiles\", \"inchikey1\", \"inchikey1_ref\", \"nn\", \"score\"]].to_csv(short_out_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = msc.get_candidate_scores()\n",
    "results_ok = msc.compute_candidate_scores(results_ok, fp_map, \n",
    "                                          additive_smoothing_n = n_total_,\n",
    "                                          f1_cutoff = f1_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score_mod_platt': <function infrastructure.score.score_mod_platt(predicted, candidate, stats, f1_cutoff=0.5)>,\n",
       " 'score_unit': <function infrastructure.score.score_unit(predicted, candidate, stats=None, f1_cutoff=0.5)>,\n",
       " 'score_unit_pos': <function infrastructure.score.score_unit_pos(predicted, candidate, stats=None, f1_cutoff=0.5)>,\n",
       " 'score_platt': <function infrastructure.score.score_platt(predicted, candidate, stats=None, f1_cutoff=0.5)>,\n",
       " 'score_max_likelihood': <function infrastructure.score.score_max_likelihood(predicted, candidate, stats, f1_cutoff=0.5)>,\n",
       " 'score_tanimoto': <function infrastructure.score.score_tanimoto(predicted, candidate, stats=None, f1_cutoff=0.5)>,\n",
       " 'score_rel_mod_platt': <function infrastructure.score.score_rel_mod_platt(predicted, candidate, stats, f1_cutoff=0.5)>,\n",
       " 'score_lim_mod_platt': <function infrastructure.score.score_lim_mod_platt(predicted, candidate, stats, f1_cutoff=0.5)>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/MSNovelist-data/fingerprint_map_pseudo.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-35c4bd006f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfp_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFingerprintMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fp_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/msnovelist/fp_management/fingerprint_map.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, data, subset, explicit_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No file or dataframe supplied.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/MSNovelist-data/fingerprint_map_pseudo.tsv'"
     ]
    }
   ],
   "source": [
    "\n",
    "fp_map = fpm.FingerprintMap(sc.config[\"fp_map\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
